{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import praw\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 3 subreddits we are interested in: r/Bitcoin, r/BitcoinBeginners, r/BitcoinMarkets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data for 10 submissions.\n",
      "Saved data for 20 submissions.\n",
      "Saved data for 30 submissions.\n",
      "Saved data for 40 submissions.\n",
      "Saved data for 50 submissions.\n",
      "Saved data for 60 submissions.\n",
      "Saved data for 70 submissions.\n",
      "Saved data for 80 submissions.\n",
      "Saved data for 90 submissions.\n",
      "Saved data for 100 submissions.\n",
      "Saved data for 110 submissions.\n",
      "Saved data for 120 submissions.\n",
      "Saved data for 130 submissions.\n",
      "Saved data for 140 submissions.\n",
      "Saved data for 150 submissions.\n",
      "Saved data for 160 submissions.\n",
      "Saved data for 170 submissions.\n",
      "Saved data for 180 submissions.\n",
      "Saved data for 190 submissions.\n",
      "Saved data for 200 submissions.\n",
      "Saved data for 210 submissions.\n",
      "Saved data for 220 submissions.\n",
      "Saved data for 230 submissions.\n",
      "Saved data for 240 submissions.\n",
      "Saved data for 250 submissions.\n",
      "Saved data for 260 submissions.\n",
      "Saved data for 270 submissions.\n",
      "Saved data for 280 submissions.\n",
      "Saved data for 290 submissions.\n",
      "Saved data for 300 submissions.\n",
      "Saved data for 310 submissions.\n",
      "Saved data for 320 submissions.\n",
      "Saved data for 330 submissions.\n",
      "Saved data for 340 submissions.\n",
      "Saved data for 350 submissions.\n",
      "Saved data for 360 submissions.\n",
      "Saved data for 370 submissions.\n",
      "Saved data for 380 submissions.\n",
      "Saved data for 390 submissions.\n",
      "Saved data for 400 submissions.\n",
      "Saved data for 410 submissions.\n",
      "Saved data for 420 submissions.\n",
      "Saved data for 430 submissions.\n",
      "Saved data for 440 submissions.\n",
      "Saved data for 450 submissions.\n",
      "Saved data for 460 submissions.\n",
      "Saved data for 470 submissions.\n",
      "Saved data for 480 submissions.\n",
      "Saved data for 490 submissions.\n",
      "Saved data for 500 submissions.\n",
      "Saved data for 510 submissions.\n",
      "Saved data for 520 submissions.\n",
      "Saved data for 530 submissions.\n",
      "Saved data for 540 submissions.\n",
      "Saved data for 550 submissions.\n",
      "Saved data for 560 submissions.\n",
      "Saved data for 570 submissions.\n",
      "Saved data for 580 submissions.\n",
      "Saved data for 590 submissions.\n",
      "Saved data for 600 submissions.\n",
      "Saved data for 610 submissions.\n",
      "Saved data for 620 submissions.\n",
      "Saved data for 630 submissions.\n",
      "Saved data for 640 submissions.\n",
      "Saved data for 650 submissions.\n",
      "Saved data for 660 submissions.\n",
      "Saved data for 670 submissions.\n",
      "Saved data for 680 submissions.\n",
      "Saved data for 690 submissions.\n",
      "Saved data for 700 submissions.\n",
      "Saved data for 710 submissions.\n",
      "Saved data for 720 submissions.\n",
      "Saved data for 730 submissions.\n",
      "Saved data for 740 submissions.\n",
      "Saved data for 750 submissions.\n",
      "Saved data for 760 submissions.\n",
      "Saved data for 770 submissions.\n",
      "Saved data for 780 submissions.\n",
      "Saved data for 790 submissions.\n",
      "Saved data for 800 submissions.\n",
      "Saved data for 810 submissions.\n",
      "Saved data for 820 submissions.\n",
      "Saved data for 830 submissions.\n",
      "Saved data for 840 submissions.\n",
      "Saved data for 850 submissions.\n",
      "Saved data for 860 submissions.\n",
      "Saved data for 870 submissions.\n",
      "Saved data for 880 submissions.\n",
      "Saved data for 890 submissions.\n",
      "Saved data for 900 submissions.\n",
      "Saved data for 910 submissions.\n",
      "Saved data for 920 submissions.\n",
      "Saved data for 930 submissions.\n",
      "Saved data for 940 submissions.\n",
      "Saved data for 950 submissions.\n",
      "Saved data for 960 submissions.\n",
      "Saved data for 970 submissions.\n",
      "Saved data for 980 submissions.\n",
      "Saved data for 990 submissions.\n"
     ]
    }
   ],
   "source": [
    "def getConfig():\n",
    "    with open('config.json') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "config = getConfig()\n",
    "    \n",
    "# Setup PRAW\n",
    "reddit = praw.Reddit(\n",
    "    client_id=config['client_id'],\n",
    "    client_secret=config['client_secret'],\n",
    "    user_agent=config['user_agent']\n",
    ")\n",
    "\n",
    "\n",
    "def get_submission_info(submission):\n",
    "    \"\"\"Extract desired information from a submission object.\"\"\"\n",
    "    return {\n",
    "        \"author\": submission.author.name if submission.author else \"[deleted]\",\n",
    "        \"created_utc\": datetime.fromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        \"distinguished\": submission.distinguished,\n",
    "        \"edited\": submission.edited if isinstance(submission.edited, bool) else bool(submission.edited),\n",
    "        \"is_original_content\": submission.is_original_content,\n",
    "        \"is_self\": submission.is_self,\n",
    "        \"link_flair_text\": submission.link_flair_text,\n",
    "        \"locked\": submission.locked,\n",
    "        \"nsfw\": submission.over_18,\n",
    "        \"num_comments\": submission.num_comments,\n",
    "        \"permalink\": submission.permalink,\n",
    "        \"score\": submission.score,\n",
    "        \"selftext\": submission.selftext,\n",
    "        \"spoiler\": submission.spoiler,\n",
    "        \"stickied\": submission.stickied,\n",
    "        \"subreddit\": submission.subreddit.display_name,\n",
    "        \"title\": submission.title,\n",
    "        \"upvote_ratio\": submission.upvote_ratio\n",
    "    }\n",
    "\n",
    "def get_top_submissions_and_comments(subreddit_name, start_year, end_year, limit=100, save_interval=10, filename='reddit_comments_data.csv'):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    all_data = []\n",
    "    submission_count = 0\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        start_timestamp = int(datetime(year, 1, 1).timestamp())\n",
    "        end_timestamp = int(datetime(year, 12, 31).timestamp())\n",
    "\n",
    "        # Iterate over the top submissions\n",
    "        try:\n",
    "            for submission in subreddit.top(limit=None):\n",
    "                if start_timestamp <= submission.created_utc <= end_timestamp:\n",
    "                    submission_info = get_submission_info(submission)\n",
    "                    \n",
    "                    # Extract comments for each submission\n",
    "                    submission.comments.replace_more(limit=None)\n",
    "                    for comment in submission.comments.list():\n",
    "                        comment_info = submission_info.copy()  # Start with submission info\n",
    "                        comment_info.update({\n",
    "                            \"author\": comment.author.name if comment.author else \"[deleted]\",\n",
    "                            \"created_utc\": datetime.fromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                            \"distinguished\": comment.distinguished,\n",
    "                            \"edited\": comment.edited if isinstance(comment.edited, bool) else bool(comment.edited),\n",
    "                            \"is_original_content\": False,  # Comments are not original content\n",
    "                            \"is_self\": False,  # Comments are not self-posts\n",
    "                            \"link_flair_text\": None,  # Comments don't have link flair\n",
    "                            \"locked\": False,  # Comments are not locked\n",
    "                            \"nsfw\": False,  # Comments don't have an NSFW flag\n",
    "                            \"num_comments\": 0,  # Comments don't have comment counts\n",
    "                            \"permalink\": f\"https://www.reddit.com{comment.permalink}\",\n",
    "                            \"score\": comment.score,\n",
    "                            \"selftext\": comment.body,\n",
    "                            \"spoiler\": False,  # Comments don't have a spoiler flag\n",
    "                            \"stickied\": comment.stickied,\n",
    "                            \"subreddit\": subreddit_name,\n",
    "                            \"title\": submission.title,\n",
    "                            \"upvote_ratio\": submission.upvote_ratio  # Using the upvote ratio of the submission\n",
    "                        })\n",
    "                        all_data.append(comment_info)\n",
    "                    \n",
    "                    submission_count += 1\n",
    "\n",
    "                    # Periodically save data to CSV\n",
    "                    if submission_count % save_interval == 0:\n",
    "                        save_to_csv(all_data, filename)\n",
    "                        print(f\"Saved data for {submission_count} submissions.\")\n",
    "                        all_data.clear()  # Clear the list after saving\n",
    "\n",
    "                    # Respect Reddit's rate limit\n",
    "                    time.sleep(1)\n",
    "            \n",
    "        except praw.exceptions.APIException as e:\n",
    "            print(f\"API Exception encountered: {e}\")\n",
    "            time.sleep(10)  # Wait and retry\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            time.sleep(5)  # Wait and retry\n",
    "\n",
    "    # Save any remaining data\n",
    "    if all_data:\n",
    "        save_to_csv(all_data, filename)\n",
    "        print(f\"Final save: {submission_count} submissions processed.\")\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    headers = [\n",
    "        \"author\", \"created_utc\", \"distinguished\", \"edited\", \"is_original_content\", \n",
    "        \"is_self\", \"link_flair_text\", \"locked\", \"nsfw\", \"num_comments\", \"permalink\", \n",
    "        \"score\", \"selftext\", \"spoiler\", \"stickied\", \"subreddit\", \"title\", \"upvote_ratio\"\n",
    "    ]\n",
    "    \n",
    "    with open(filename, mode='a', newline='', encoding='utf-8') as file:  # Use 'a' mode to append to the file\n",
    "        writer = csv.DictWriter(file, fieldnames=headers)\n",
    "        if file.tell() == 0:  # If the file is empty, write the header\n",
    "            writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "\n",
    "subreddit_name = 'BitcoinBeginners' #Change to 'BitcoinBeginners' or 'BitcoinMarkets' to get data for those subreddits\n",
    "get_top_submissions_and_comments(subreddit_name, 2017, 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(562196, 18)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_bitcoin = pd.read_excel('ANL488_Git/Data/reddit_comments_data_bitcoin.xlsx')\n",
    "\n",
    "reddit_bitcoinBeginners = pd.read_csv('ANL488_Git/Data/reddit_comments_data_bitcoinbeginners.csv')\n",
    "\n",
    "reddit_bitcoinMarkets = pd.read_csv('ANL488_Git/Data/reddit_comments_data_bitcoinmarkets.csv')\n",
    "\n",
    "combined_reddit = pd.concat([reddit_bitcoin, reddit_bitcoinBeginners, reddit_bitcoinMarkets])\n",
    "\n",
    "# see how many rows and columns are in the dataframe\n",
    "combined_reddit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the 'created_utc' column into a datetime object\n",
    "combined_reddit['created_utc'] = pd.to_datetime(combined_reddit['created_utc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>edited</th>\n",
       "      <th>is_original_content</th>\n",
       "      <th>is_self</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>locked</th>\n",
       "      <th>nsfw</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>permalink</th>\n",
       "      <th>score</th>\n",
       "      <th>selftext</th>\n",
       "      <th>spoiler</th>\n",
       "      <th>stickied</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SPOKANARCHY</td>\n",
       "      <td>2017-11-29 09:49:15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/Bitcoin/comments/7g9c...</td>\n",
       "      <td>2472</td>\n",
       "      <td>Where‚Äôs the guy that‚Äôs going to eat his le...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Bitcoin</td>\n",
       "      <td>It's official! 1 Bitcoin = $10,000 USD</td>\n",
       "      <td>0.81</td>\n",
       "      <td>2017-11-29</td>\n",
       "      <td>09:49:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TarAldarion</td>\n",
       "      <td>2017-11-29 09:48:31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/Bitcoin/comments/7g9c...</td>\n",
       "      <td>6798</td>\n",
       "      <td>It's official. 100 million dollar pizza.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Bitcoin</td>\n",
       "      <td>It's official! 1 Bitcoin = $10,000 USD</td>\n",
       "      <td>0.81</td>\n",
       "      <td>2017-11-29</td>\n",
       "      <td>09:48:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>walloon5</td>\n",
       "      <td>2017-11-29 09:31:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/Bitcoin/comments/7g9c...</td>\n",
       "      <td>2282</td>\n",
       "      <td>Wooooo!!!!\\n\\nI watched the wall fall on GDAX ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Bitcoin</td>\n",
       "      <td>It's official! 1 Bitcoin = $10,000 USD</td>\n",
       "      <td>0.81</td>\n",
       "      <td>2017-11-29</td>\n",
       "      <td>09:31:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>flclst3v3</td>\n",
       "      <td>2017-11-29 09:27:14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/Bitcoin/comments/7g9c...</td>\n",
       "      <td>2166</td>\n",
       "      <td>See you at 15k</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Bitcoin</td>\n",
       "      <td>It's official! 1 Bitcoin = $10,000 USD</td>\n",
       "      <td>0.81</td>\n",
       "      <td>2017-11-29</td>\n",
       "      <td>09:27:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ditto755</td>\n",
       "      <td>2017-11-29 09:40:44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/Bitcoin/comments/7g9c...</td>\n",
       "      <td>1672</td>\n",
       "      <td>[Full Celebration](https://www.youtube.com/wat...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Bitcoin</td>\n",
       "      <td>It's official! 1 Bitcoin = $10,000 USD</td>\n",
       "      <td>0.81</td>\n",
       "      <td>2017-11-29</td>\n",
       "      <td>09:40:44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        author         created_utc distinguished  edited  is_original_content  \\\n",
       "0  SPOKANARCHY 2017-11-29 09:49:15           NaN   False                False   \n",
       "1  TarAldarion 2017-11-29 09:48:31           NaN   False                False   \n",
       "2     walloon5 2017-11-29 09:31:00           NaN   False                False   \n",
       "3    flclst3v3 2017-11-29 09:27:14           NaN   False                False   \n",
       "4     ditto755 2017-11-29 09:40:44           NaN   False                False   \n",
       "\n",
       "   is_self  link_flair_text  locked   nsfw  num_comments  \\\n",
       "0    False              NaN   False  False             0   \n",
       "1    False              NaN   False  False             0   \n",
       "2    False              NaN   False  False             0   \n",
       "3    False              NaN   False  False             0   \n",
       "4    False              NaN   False  False             0   \n",
       "\n",
       "                                           permalink  score  \\\n",
       "0  https://www.reddit.com/r/Bitcoin/comments/7g9c...   2472   \n",
       "1  https://www.reddit.com/r/Bitcoin/comments/7g9c...   6798   \n",
       "2  https://www.reddit.com/r/Bitcoin/comments/7g9c...   2282   \n",
       "3  https://www.reddit.com/r/Bitcoin/comments/7g9c...   2166   \n",
       "4  https://www.reddit.com/r/Bitcoin/comments/7g9c...   1672   \n",
       "\n",
       "                                            selftext  spoiler  stickied  \\\n",
       "0  Where‚Äôs the guy that‚Äôs going to eat his le...    False     False   \n",
       "1           It's official. 100 million dollar pizza.    False     False   \n",
       "2  Wooooo!!!!\\n\\nI watched the wall fall on GDAX ...    False     False   \n",
       "3                                     See you at 15k    False     False   \n",
       "4  [Full Celebration](https://www.youtube.com/wat...    False     False   \n",
       "\n",
       "  subreddit                                   title  upvote_ratio        date  \\\n",
       "0   Bitcoin  It's official! 1 Bitcoin = $10,000 USD          0.81  2017-11-29   \n",
       "1   Bitcoin  It's official! 1 Bitcoin = $10,000 USD          0.81  2017-11-29   \n",
       "2   Bitcoin  It's official! 1 Bitcoin = $10,000 USD          0.81  2017-11-29   \n",
       "3   Bitcoin  It's official! 1 Bitcoin = $10,000 USD          0.81  2017-11-29   \n",
       "4   Bitcoin  It's official! 1 Bitcoin = $10,000 USD          0.81  2017-11-29   \n",
       "\n",
       "       time  \n",
       "0  09:49:15  \n",
       "1  09:48:31  \n",
       "2  09:31:00  \n",
       "3  09:27:14  \n",
       "4  09:40:44  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date = combined_reddit['created_utc'].dt.date\n",
    "time = combined_reddit['created_utc'].dt.time\n",
    "\n",
    "# create a new column for the date\n",
    "combined_reddit['date'] = date\n",
    "\n",
    "# create a new column for the time\n",
    "combined_reddit['time'] = time\n",
    "\n",
    "combined_reddit.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import TextBlob\n",
    "import emoji\n",
    "from datetime import datetime\n",
    "import re\n",
    "from transformers import  BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/bokyannchou/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/bokyannchou/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# downloading the stopwords and supporting libraries for text preprocessing\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing only required columns\n",
    "final_data_sentiment = combined_reddit[['date','time','selftext', 'title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/20/tq939v215g77379rg46gb1dc0000gn/T/ipykernel_37399/1654923122.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_data_sentiment['selftext'] = final_data_sentiment['selftext'].astype(str)\n",
      "/var/folders/20/tq939v215g77379rg46gb1dc0000gn/T/ipykernel_37399/1654923122.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_data_sentiment['title'] = final_data_sentiment['title'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "# make sure the 'selftext' column is a string\n",
    "final_data_sentiment['selftext'] = final_data_sentiment['selftext'].astype(str)\n",
    "final_data_sentiment['title'] = final_data_sentiment['title'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/20/tq939v215g77379rg46gb1dc0000gn/T/ipykernel_37399/149024960.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_data_sentiment['selftext_cleaned'] = final_data_sentiment['selftext'].apply(preprocess_text)\n",
      "/var/folders/20/tq939v215g77379rg46gb1dc0000gn/T/ipykernel_37399/149024960.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_data_sentiment['title_cleaned'] = final_data_sentiment['title'].apply(preprocess_text)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the text\n",
    "def preprocess_text(text):\n",
    "\n",
    "    # Remove non-alphanumeric characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "    # lower the text\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "final_data_sentiment['selftext_cleaned'] = final_data_sentiment['selftext'].apply(preprocess_text)\n",
    "final_data_sentiment['title_cleaned'] = final_data_sentiment['title'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(datetime.date(2017, 1, 4), datetime.date(2024, 9, 8))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final_data_sentiment date range\n",
    "final_data_sentiment['date'].min(), final_data_sentiment['date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/20/tq939v215g77379rg46gb1dc0000gn/T/ipykernel_37399/281366849.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_data_sentiment['hour'] = final_data_sentiment['time'].apply(lambda x: x.hour)\n"
     ]
    }
   ],
   "source": [
    "final_data_sentiment['hour'] = final_data_sentiment['time'].apply(lambda x: x.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tensortrade/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/var/folders/20/tq939v215g77379rg46gb1dc0000gn/T/ipykernel_37399/2008759358.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_data_sentiment['combined_text'] = final_data_sentiment.apply(combine_title_comment, axis=1)\n",
      "/var/folders/20/tq939v215g77379rg46gb1dc0000gn/T/ipykernel_37399/2008759358.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_data_sentiment['datetime'] = final_data_sentiment.apply(lambda x: datetime.combine(x['date'], x['time']), axis=1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdeaf2097b4c41f49722a766560645ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/562196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47463d84a8114923885d28d344cc7e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/562196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2647 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             date  hour  sentiment_score_avg  positive_count  negative_count\n",
      "0      2017-01-04    12             0.949070              11              21\n",
      "1      2017-01-04    13             0.935518              22              50\n",
      "2      2017-01-04    14             0.906919              13              22\n",
      "3      2017-01-04    15             0.955167               5              12\n",
      "4      2017-01-04    16             0.956778               7              26\n",
      "...           ...   ...                  ...             ...             ...\n",
      "28071  2024-09-07    22             0.998383               0               3\n",
      "28072  2024-09-07    23             0.997304               0               6\n",
      "28073  2024-09-08     2             0.999295               0               1\n",
      "28074  2024-09-08     3             0.994338               0               3\n",
      "28075  2024-09-08     6             0.999112               0               1\n",
      "\n",
      "[28076 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "# Configure logging to track errors\n",
    "logging.basicConfig(filename='error_log.log', level=logging.ERROR)\n",
    "\n",
    "# Use a lighter, faster model (DistilBERT)\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Sentiment-analysis pipeline\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "\n",
    "# Combine title and comment\n",
    "def combine_title_comment(row):\n",
    "    return f\"{row['title_cleaned']} [SEP] {row['selftext_cleaned']}\"\n",
    "\n",
    "final_data_sentiment['combined_text'] = final_data_sentiment.apply(combine_title_comment, axis=1)\n",
    "\n",
    "# Combine date and time to form datetime, both is already in datetime format\n",
    "final_data_sentiment['datetime'] = final_data_sentiment.apply(lambda x: datetime.combine(x['date'], x['time']), axis=1)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(final_data_sentiment[['combined_text', 'datetime']])\n",
    "\n",
    "# Truncate input length to 128 tokens for faster processing\n",
    "def tokenize_function(batch):\n",
    "    return tokenizer(batch['combined_text'], truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Tokenize the dataset\n",
    "dataset = dataset.map(tokenize_function, batched=True, batch_size=128)\n",
    "\n",
    "# Perform sentiment analysis with batch processing and error handling\n",
    "def analyze_sentiment(batch):\n",
    "    try:\n",
    "        results = sentiment_pipeline(batch['combined_text'])\n",
    "        batch['sentiment'] = [res['label'] for res in results]\n",
    "        batch['score'] = [res['score'] for res in results]\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Sentiment analysis error: {e}\")\n",
    "        batch['sentiment'] = [\"ERROR\"] * len(batch['combined_text'])\n",
    "        batch['score'] = [0.0] * len(batch['combined_text'])\n",
    "    return batch\n",
    "\n",
    "# Process in larger batches (increase batch size to 64 or 128)\n",
    "dataset = dataset.map(analyze_sentiment, batched=True, batch_size=64)\n",
    "\n",
    "# Aggregate sentiment by date\n",
    "df_results = pd.DataFrame(dataset)\n",
    "\n",
    "# Handle rows where there were errors in sentiment analysis\n",
    "df_results = df_results[df_results['sentiment'] != 'ERROR']\n",
    "\n",
    "# Aggregate sentiment by date and hour\n",
    "df_results['datetime'] = pd.to_datetime(df_results['datetime'])\n",
    "df_results['hour'] = df_results['datetime'].dt.hour\n",
    "df_results['date'] = df_results['datetime'].dt.date\n",
    "\n",
    "df_agg = df_results.groupby(['date', 'hour']).agg(\n",
    "    sentiment_score_avg=('score', 'mean'),\n",
    "    positive_count=('sentiment', lambda x: sum(1 for label in x if label == 'POSITIVE')),\n",
    "    negative_count=('sentiment', lambda x: sum(1 for label in x if label == 'NEGATIVE'))\n",
    ").reset_index()\n",
    "\n",
    "print(df_agg)\n",
    "\n",
    "# Save the results\n",
    "df_agg.to_csv('Data/reddit_sentiment_analysis.csv', index=False)\n",
    "\n",
    "#save as parquet file as well\n",
    "df_agg.to_parquet('Data/reddit_sentiment_analysis.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
